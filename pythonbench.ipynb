{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff11c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d742976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mandelbrot(c, max_iter):\n",
    "    z = 0\n",
    "    n = 0\n",
    "    while abs(z) <= 2 and n < max_iter:\n",
    "        z = z**2 + c\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "def mandelbrot_set(width, height, x_min, x_max, y_min, y_max, max_iter):\n",
    "    result = []\n",
    "    for y in range(height):\n",
    "        row = []\n",
    "        for x in range(width):\n",
    "            real = x_min + (x / width) * (x_max - x_min)\n",
    "            imag = y_min + (y / height) * (y_max - y_min)\n",
    "            c = complex(real, imag)\n",
    "            row.append(mandelbrot(c, max_iter))\n",
    "        result.append(row)\n",
    "    return result\n",
    "\n",
    "width = 960\n",
    "height = 960\n",
    "x_min, x_max = -2, 0.6\n",
    "y_min, y_max = -1.5, 1.5\n",
    "max_iter = 200\n",
    "start_time = time.time()\n",
    "result = mandelbrot_set(width, height, x_min, x_max, y_min, y_max, max_iter)\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f311f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.441244840621948\n"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9c76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "width = 960\n",
    "height = 960\n",
    "MAX_ITERS = 200\n",
    "\n",
    "min_x = -2.0\n",
    "max_x = 0.6\n",
    "min_y = -1.5\n",
    "max_y = 1.5\n",
    "def mandelbrot_kernel(c):\n",
    "    z = c\n",
    "    for i in range(MAX_ITERS):\n",
    "        z = z * z + c\n",
    "        if abs(z) > 4:\n",
    "            return i\n",
    "    return MAX_ITERS\n",
    "\n",
    "\n",
    "def compute_mandelbrot():#Tensor[float_type]:\n",
    "    # create a matrix. Each element of the matrix corresponds to a pixel\n",
    "    start = time.time()\n",
    "\n",
    "    t = np.zeros((height, width))\n",
    "\n",
    "    dx = (max_x - min_x) / width\n",
    "    dy = (max_y - min_y) / height\n",
    "\n",
    "    y = min_y\n",
    "    for row in range(height):\n",
    "        x = min_x\n",
    "        for col in range(width):\n",
    "            t[row, col] = mandelbrot_kernel(complex(x, y))\n",
    "            x += dx\n",
    "        y += dy\n",
    "    end = time.time()\n",
    "    result =end-start\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774502ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def load_mnist_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    Y = data.iloc[:, 0].values\n",
    "    X = data.iloc[:, 1:].values\n",
    "    X = X / 255.0  # Normalize pixel values\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c004ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.302563784064752\n",
      "Epoch 10, Loss: 2.302563382721802\n",
      "Epoch 20, Loss: 2.3025629814492627\n",
      "Epoch 30, Loss: 2.302562580247123\n",
      "Epoch 40, Loss: 2.3025621791153665\n",
      "Epoch 50, Loss: 2.30256177805398\n",
      "Epoch 60, Loss: 2.30256137706295\n",
      "Epoch 70, Loss: 2.3025609761422623\n",
      "Epoch 80, Loss: 2.3025605752919027\n",
      "Epoch 90, Loss: 2.3025601745118585\n",
      "Training time: 5.4561567306518555 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size1, hidden_size2, output_size):\n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    parameters['W1'] = np.random.randn(input_size, hidden_size1) * 0.01\n",
    "    parameters['b1'] = np.zeros((1, hidden_size1))\n",
    "    parameters['W2'] = np.random.randn(hidden_size1, hidden_size2) * 0.01\n",
    "    parameters['b2'] = np.zeros((1, hidden_size2))\n",
    "    parameters['W3'] = np.random.randn(hidden_size2, output_size) * 0.01\n",
    "    parameters['b3'] = np.zeros((1, output_size))\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    Z1 = np.dot(X, parameters['W1']) + parameters['b1']\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, parameters['W2']) + parameters['b2']\n",
    "    A2 = tanh(Z2)\n",
    "    Z3 = np.dot(A2, parameters['W3']) + parameters['b3']\n",
    "    A3 = softmax(Z3)\n",
    "    return {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2, 'Z3': Z3, 'A3': A3}\n",
    "\n",
    "def compute_loss(Y, probs):\n",
    "    m = Y.shape[0]\n",
    "    log_probs = -np.log(probs[range(m), Y])\n",
    "    loss = np.sum(log_probs) / m\n",
    "    return loss\n",
    "\n",
    "def backward_propagation(X, Y, parameters, cache):\n",
    "    m = X.shape[0]\n",
    "    dZ3 = cache['A3'] - np.eye(10)[Y]\n",
    "    dW3 = np.dot(cache['A2'].T, dZ3) / m\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "    dZ2 = np.dot(dZ3, parameters['W3'].T) * (cache['A2'] * (1 - cache['A2']))\n",
    "    dW2 = np.dot(cache['A1'].T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    dZ1 = np.dot(dZ2, parameters['W2'].T) * (cache['A1'] * (1 - cache['A1']))\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    gradients = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3}\n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    parameters['W1'] -= learning_rate * gradients['dW1']\n",
    "    parameters['b1'] -= learning_rate * gradients['db1']\n",
    "    parameters['W2'] -= learning_rate * gradients['dW2']\n",
    "    parameters['b2'] -= learning_rate * gradients['db2']\n",
    "    parameters['W3'] -= learning_rate * gradients['dW3']\n",
    "    parameters['b3'] -= learning_rate * gradients['db3']\n",
    "    return parameters\n",
    "\n",
    "def train_neural_network_np(X, Y, input_size, hidden_size1, hidden_size2, output_size, learning_rate, epochs):\n",
    "    parameters = initialize_parameters(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward Propagation\n",
    "        cache = forward_propagation(X, parameters)\n",
    "        probs = cache['A3']\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = compute_loss(Y, probs)\n",
    "\n",
    "        # Backward Propagation\n",
    "        gradients = backward_propagation(X, Y, parameters, cache)\n",
    "\n",
    "        # Update Parameters\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "        # Print Loss every 100 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\"\"\"# Dummy data for demonstration\n",
    "input_size = 784\n",
    "hidden_size1 = 150\n",
    "hidden_size2 = 80\n",
    "output_size = 10\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\"\"\"\n",
    "\"\"\"\n",
    "# Replace X and Y with your actual data\n",
    "X = np.random.randn(1000, input_size)\n",
    "Y = np.random.randint(0, 10, size=(1000,))\n",
    "start_time = time.time()\n",
    "trained_parameters = train_neural_network(X, Y, input_size, hidden_size1, hidden_size2, output_size, learning_rate, epochs)\n",
    "end = time.time()\n",
    "print(end-start_time)\"\"\"\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\PC\\\\Desktop\\\\AIframework\\\\MNIST_test.csv'\n",
    "\n",
    "X, Y = load_mnist_data(file_path)\n",
    "\n",
    "# [Your existing neural network parameters]\n",
    "input_size = 784\n",
    "hidden_size1 = 150\n",
    "hidden_size2 = 80\n",
    "output_size = 10\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "# Training the model with the MNIST data\n",
    "start_time = time.time()\n",
    "trained_parameters = train_neural_network_np(X, Y, input_size, hidden_size1, hidden_size2, output_size, learning_rate, epochs)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "422687a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0747b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.302729606628418\n",
      "Epoch 10, Loss: 2.3027291297912598\n",
      "Epoch 20, Loss: 2.3027284145355225\n",
      "Epoch 30, Loss: 2.302727699279785\n",
      "Epoch 40, Loss: 2.302727222442627\n",
      "Epoch 50, Loss: 2.3027267456054688\n",
      "Epoch 60, Loss: 2.3027260303497314\n",
      "Epoch 70, Loss: 2.302725315093994\n",
      "Epoch 80, Loss: 2.3027243614196777\n",
      "Epoch 90, Loss: 2.3027241230010986\n",
      "Training time: 1.4479990005493164 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "def train_neural_network_pt(X, Y, input_size, hidden_size1, hidden_size2, output_size, learning_rate, epochs):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    Y_tensor = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "    model = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward Propagation\n",
    "        outputs = model(X_tensor)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = criterion(outputs, Y_tensor)\n",
    "\n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print Loss every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model\n",
    "\"\"\"\n",
    "# Dummy data for demonstration\n",
    "input_size = 784\n",
    "hidden_size1 = 150\n",
    "hidden_size2 = 80\n",
    "output_size = 10\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "# Replace X and Y with your actual data\n",
    "X = np.random.randn(1000, input_size)\n",
    "Y = np.random.randint(0, 10, size=(1000,))\n",
    "start_time = time.time()\n",
    "trained_model = train_neural_network(X, Y, input_size, hidden_size1, hidden_size2, output_size, learning_rate, epochs)\n",
    "end = time.time()\n",
    "print(end - start_time)\n",
    "start_time = time.time()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\PC\\\\Desktop\\\\AIframework\\\\MNIST_test.csv'\n",
    "\n",
    "X, Y = load_mnist_data(file_path)\n",
    "\n",
    "# [Your existing neural network parameters]\n",
    "input_size = 784\n",
    "hidden_size1 = 150\n",
    "hidden_size2 = 80\n",
    "output_size = 10\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "# Training the model with the MNIST data\n",
    "start_time = time.time()\n",
    "trained_parameters = train_neural_network_pt(X, Y, input_size, hidden_size1, hidden_size2, output_size, learning_rate, epochs)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43c9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "313/313 [==============================] - 1s 821us/step - loss: 2.3904 - accuracy: 0.1400\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 0s 760us/step - loss: 2.3468 - accuracy: 0.1502\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 0s 782us/step - loss: 2.3057 - accuracy: 0.1623\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 0s 760us/step - loss: 2.2669 - accuracy: 0.1790\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 0s 769us/step - loss: 2.2298 - accuracy: 0.1960\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 0s 753us/step - loss: 2.1944 - accuracy: 0.2148\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 0s 754us/step - loss: 2.1603 - accuracy: 0.2404\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 0s 763us/step - loss: 2.1273 - accuracy: 0.2691\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 0s 745us/step - loss: 2.0953 - accuracy: 0.3039\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 0s 753us/step - loss: 2.0641 - accuracy: 0.3399\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 0s 747us/step - loss: 2.0337 - accuracy: 0.3754\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 0s 751us/step - loss: 2.0038 - accuracy: 0.4098\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 0s 772us/step - loss: 1.9746 - accuracy: 0.4456\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 0s 760us/step - loss: 1.9459 - accuracy: 0.4756\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 0s 753us/step - loss: 1.9178 - accuracy: 0.5050\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 0s 764us/step - loss: 1.8901 - accuracy: 0.5321\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 0s 779us/step - loss: 1.8628 - accuracy: 0.5510\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 0s 795us/step - loss: 1.8360 - accuracy: 0.5722\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 0s 760us/step - loss: 1.8097 - accuracy: 0.5879\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 0s 769us/step - loss: 1.7837 - accuracy: 0.6037\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 0s 779us/step - loss: 1.7581 - accuracy: 0.6169\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 0s 779us/step - loss: 1.7328 - accuracy: 0.6293\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 0s 763us/step - loss: 1.7080 - accuracy: 0.6415\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 0s 756us/step - loss: 1.6835 - accuracy: 0.6538\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 0s 756us/step - loss: 1.6593 - accuracy: 0.6648\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 0s 772us/step - loss: 1.6356 - accuracy: 0.6743\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 0s 772us/step - loss: 1.6121 - accuracy: 0.6828\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 0s 769us/step - loss: 1.5891 - accuracy: 0.6910\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 0s 760us/step - loss: 1.5664 - accuracy: 0.6985\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 0s 772us/step - loss: 1.5441 - accuracy: 0.7075\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 0s 763us/step - loss: 1.5221 - accuracy: 0.7145\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 0s 760us/step - loss: 1.5006 - accuracy: 0.7201\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 0s 779us/step - loss: 1.4793 - accuracy: 0.7266\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 0s 795us/step - loss: 1.4585 - accuracy: 0.7316\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 0s 772us/step - loss: 1.4381 - accuracy: 0.7364\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 0s 756us/step - loss: 1.4180 - accuracy: 0.7413\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 0s 766us/step - loss: 1.3983 - accuracy: 0.7462\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 0s 772us/step - loss: 1.3790 - accuracy: 0.7502\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 0s 761us/step - loss: 1.3600 - accuracy: 0.7540\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 0s 769us/step - loss: 1.3414 - accuracy: 0.7575\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 0s 761us/step - loss: 1.3232 - accuracy: 0.7609\n",
      "Epoch 42/100\n",
      "313/313 [==============================] - 0s 760us/step - loss: 1.3053 - accuracy: 0.7649\n",
      "Epoch 43/100\n",
      "313/313 [==============================] - 0s 766us/step - loss: 1.2879 - accuracy: 0.7683\n",
      "Epoch 44/100\n",
      "313/313 [==============================] - 0s 772us/step - loss: 1.2708 - accuracy: 0.7706\n",
      "Epoch 45/100\n",
      "313/313 [==============================] - 0s 769us/step - loss: 1.2540 - accuracy: 0.7727\n",
      "Epoch 46/100\n",
      "313/313 [==============================] - 0s 787us/step - loss: 1.2376 - accuracy: 0.7759\n",
      "Epoch 47/100\n",
      "313/313 [==============================] - 0s 795us/step - loss: 1.2216 - accuracy: 0.7783\n",
      "Epoch 48/100\n",
      "313/313 [==============================] - 0s 779us/step - loss: 1.2059 - accuracy: 0.7806\n",
      "Epoch 49/100\n",
      "313/313 [==============================] - 0s 788us/step - loss: 1.1906 - accuracy: 0.7837\n",
      "Epoch 50/100\n",
      "313/313 [==============================] - 0s 779us/step - loss: 1.1756 - accuracy: 0.7861\n",
      "Epoch 51/100\n",
      "313/313 [==============================] - 0s 795us/step - loss: 1.1610 - accuracy: 0.7873\n",
      "Epoch 52/100\n",
      "313/313 [==============================] - 0s 785us/step - loss: 1.1467 - accuracy: 0.7895\n",
      "Epoch 53/100\n",
      "313/313 [==============================] - 0s 774us/step - loss: 1.1327 - accuracy: 0.7930\n",
      "Epoch 54/100\n",
      "313/313 [==============================] - 0s 776us/step - loss: 1.1190 - accuracy: 0.7945\n",
      "Epoch 55/100\n",
      "313/313 [==============================] - 0s 794us/step - loss: 1.1057 - accuracy: 0.7967\n",
      "Epoch 56/100\n",
      "313/313 [==============================] - 0s 782us/step - loss: 1.0927 - accuracy: 0.7982\n",
      "Epoch 57/100\n",
      "313/313 [==============================] - 0s 782us/step - loss: 1.0800 - accuracy: 0.8002\n",
      "Epoch 58/100\n",
      "313/313 [==============================] - 0s 785us/step - loss: 1.0675 - accuracy: 0.8019\n",
      "Epoch 59/100\n",
      "313/313 [==============================] - 0s 792us/step - loss: 1.0554 - accuracy: 0.8034\n",
      "Epoch 60/100\n",
      "313/313 [==============================] - 0s 805us/step - loss: 1.0436 - accuracy: 0.8045\n",
      "Epoch 61/100\n",
      "313/313 [==============================] - 0s 782us/step - loss: 1.0320 - accuracy: 0.8055\n",
      "Epoch 62/100\n",
      "313/313 [==============================] - 0s 792us/step - loss: 1.0207 - accuracy: 0.8076\n",
      "Epoch 63/100\n",
      "313/313 [==============================] - 0s 790us/step - loss: 1.0097 - accuracy: 0.8089\n",
      "Epoch 64/100\n",
      "313/313 [==============================] - 0s 788us/step - loss: 0.9989 - accuracy: 0.8108\n",
      "Epoch 65/100\n",
      "313/313 [==============================] - 0s 776us/step - loss: 0.9884 - accuracy: 0.8124\n",
      "Epoch 66/100\n",
      "313/313 [==============================] - 0s 785us/step - loss: 0.9782 - accuracy: 0.8134\n",
      "Epoch 67/100\n",
      "313/313 [==============================] - 0s 789us/step - loss: 0.9681 - accuracy: 0.8150\n",
      "Epoch 68/100\n",
      "313/313 [==============================] - 0s 782us/step - loss: 0.9583 - accuracy: 0.8162\n",
      "Epoch 69/100\n",
      "313/313 [==============================] - 0s 789us/step - loss: 0.9488 - accuracy: 0.8170\n",
      "Epoch 70/100\n",
      "313/313 [==============================] - 0s 785us/step - loss: 0.9394 - accuracy: 0.8181\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 801us/step - loss: 0.9303 - accuracy: 0.8187\n",
      "Epoch 72/100\n",
      "313/313 [==============================] - 0s 782us/step - loss: 0.9214 - accuracy: 0.8201\n",
      "Epoch 73/100\n",
      "313/313 [==============================] - 0s 780us/step - loss: 0.9127 - accuracy: 0.8211\n",
      "Epoch 74/100\n",
      "313/313 [==============================] - 0s 782us/step - loss: 0.9042 - accuracy: 0.8215\n",
      "Epoch 75/100\n",
      "313/313 [==============================] - 0s 795us/step - loss: 0.8958 - accuracy: 0.8234\n",
      "Epoch 76/100\n",
      "313/313 [==============================] - 0s 795us/step - loss: 0.8877 - accuracy: 0.8244\n",
      "Epoch 77/100\n",
      "313/313 [==============================] - 0s 785us/step - loss: 0.8797 - accuracy: 0.8258\n",
      "Epoch 78/100\n",
      "313/313 [==============================] - 0s 792us/step - loss: 0.8719 - accuracy: 0.8265\n",
      "Epoch 79/100\n",
      "313/313 [==============================] - 0s 795us/step - loss: 0.8643 - accuracy: 0.8281\n",
      "Epoch 80/100\n",
      "313/313 [==============================] - 0s 785us/step - loss: 0.8569 - accuracy: 0.8289\n",
      "Epoch 81/100\n",
      "313/313 [==============================] - 0s 792us/step - loss: 0.8496 - accuracy: 0.8297\n",
      "Epoch 82/100\n",
      "313/313 [==============================] - 0s 774us/step - loss: 0.8424 - accuracy: 0.8310\n",
      "Epoch 83/100\n",
      "313/313 [==============================] - 0s 779us/step - loss: 0.8354 - accuracy: 0.8319\n",
      "Epoch 84/100\n",
      "313/313 [==============================] - 0s 779us/step - loss: 0.8286 - accuracy: 0.8327\n",
      "Epoch 85/100\n",
      "313/313 [==============================] - 0s 785us/step - loss: 0.8219 - accuracy: 0.8336\n",
      "Epoch 86/100\n",
      "313/313 [==============================] - 0s 776us/step - loss: 0.8154 - accuracy: 0.8351\n",
      "Epoch 87/100\n",
      "313/313 [==============================] - 0s 804us/step - loss: 0.8090 - accuracy: 0.8358\n",
      "Epoch 88/100\n",
      "313/313 [==============================] - 0s 792us/step - loss: 0.8027 - accuracy: 0.8364\n",
      "Epoch 89/100\n",
      "313/313 [==============================] - 0s 795us/step - loss: 0.7965 - accuracy: 0.8371\n",
      "Epoch 90/100\n",
      "313/313 [==============================] - 0s 782us/step - loss: 0.7905 - accuracy: 0.8378\n",
      "Epoch 91/100\n",
      "313/313 [==============================] - 0s 788us/step - loss: 0.7846 - accuracy: 0.8382\n",
      "Epoch 92/100\n",
      "313/313 [==============================] - 0s 785us/step - loss: 0.7788 - accuracy: 0.8392\n",
      "Epoch 93/100\n",
      "313/313 [==============================] - 0s 786us/step - loss: 0.7732 - accuracy: 0.8395\n",
      "Epoch 94/100\n",
      "313/313 [==============================] - 0s 782us/step - loss: 0.7676 - accuracy: 0.8402\n",
      "Epoch 95/100\n",
      "313/313 [==============================] - 0s 792us/step - loss: 0.7622 - accuracy: 0.8413\n",
      "Epoch 96/100\n",
      "313/313 [==============================] - 0s 795us/step - loss: 0.7568 - accuracy: 0.8418\n",
      "Epoch 97/100\n",
      "313/313 [==============================] - 0s 776us/step - loss: 0.7516 - accuracy: 0.8425\n",
      "Epoch 98/100\n",
      "313/313 [==============================] - 0s 776us/step - loss: 0.7465 - accuracy: 0.8433\n",
      "Epoch 99/100\n",
      "313/313 [==============================] - 0s 795us/step - loss: 0.7414 - accuracy: 0.8437\n",
      "Epoch 100/100\n",
      "313/313 [==============================] - 0s 788us/step - loss: 0.7365 - accuracy: 0.8447\n",
      "Training time: 25.14705181121826 seconds\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "def create_neural_network(input_size, hidden_size1, hidden_size2, output_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_size1, activation='relu', input_shape=(input_size,)),\n",
    "        tf.keras.layers.Dense(hidden_size2, activation='tanh'),\n",
    "        tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_neural_network_tf(X, Y, input_size, hidden_size1, hidden_size2, output_size, learning_rate, epochs):\n",
    "    model = create_neural_network(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X, Y, epochs=epochs, batch_size=32)\n",
    "\n",
    "    return model\n",
    "\"\"\"\n",
    "# Dummy data for demonstration\n",
    "input_size = 784\n",
    "hidden_size1 = 150\n",
    "hidden_size2 = 80\n",
    "output_size = 10\n",
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "\n",
    "# Replace X and Y with your actual data\n",
    "X = np.random.randn(1000, input_size)\n",
    "Y = np.random.randint(0, 10, size=(1000,))\n",
    "start_time = time.time()\n",
    "# Train the neural network\n",
    "model = train_neural_network(X, Y, input_size, hidden_size1, hidden_size2, output_size, learning_rate, epochs)\n",
    "end = time.time()\n",
    "print(end - start_time)\n",
    "\"\"\"\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\PC\\\\Desktop\\\\AIframework\\\\MNIST_test.csv'\n",
    "\n",
    "X, Y = load_mnist_data(file_path)\n",
    "\n",
    "# [Your existing neural network parameters]\n",
    "input_size = 784\n",
    "hidden_size1 = 150\n",
    "hidden_size2 = 80\n",
    "output_size = 10\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "# Training the model with the MNIST data\n",
    "start_time = time.time()\n",
    "trained_parameters = train_neural_network_tf(X, Y, input_size, hidden_size1, hidden_size2, output_size, learning_rate, epochs)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
